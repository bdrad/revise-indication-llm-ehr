{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3871243d",
   "metadata": {},
   "source": [
    "# Reader Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ee0b659-1ad8-429a-9aaa-238703cff299",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec79974c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_date(timestamp):\n",
    "    dt = pd.Timestamp(timestamp)\n",
    "    return f\"{dt.strftime('%B')} {dt.day}, {dt.year} - {dt.strftime('%I:%M %p')}\"\n",
    "\n",
    "def generate_note_title(enc_dept_name, note_type, auth_prov_type, deid_service_date):\n",
    "    deid_service_date = clean_date(deid_service_date) if deid_service_date else \"\"\n",
    "    if enc_dept_name and note_type and auth_prov_type and deid_service_date:\n",
    "        return f\"{enc_dept_name} - {note_type} ({auth_prov_type}) | {deid_service_date}\"\n",
    "    elif enc_dept_name and note_type and deid_service_date:\n",
    "        return f\"{enc_dept_name} - {note_type} | {deid_service_date}\"\n",
    "    elif note_type and deid_service_date:\n",
    "        return f\"{note_type} | {deid_service_date}\"\n",
    "    elif deid_service_date:\n",
    "        return f\"Clinical Note | {deid_service_date}\"\n",
    "    return f\"Clinical Note\"\n",
    "\n",
    "def generate_note_titles(row):\n",
    "    return [generate_note_title(ed, nt, apt, ds)\n",
    "            for ed, nt, apt, ds in zip(row['enc_dept_names'],\n",
    "                                        row['note_types'],\n",
    "                                        row['auth_prov_types'],\n",
    "                                        row['deid_service_dates'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8de91bd-544a-4bde-aa22-18486469078b",
   "metadata": {},
   "source": [
    "# Reader Study Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0eba0e1-7d91-41a1-bed1-07b40a85b803",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_evaluation_dataset = pd.read_parquet(\"reader_evaluation_dataset.parquet\")\n",
    "reader_evaluation_dataset[\"note_titles\"] = reader_evaluation_dataset.apply(generate_note_titles, axis=1)\n",
    "\n",
    "claude_evaluation = pd.read_csv(\"../inference/results/reader_evaluation_dataset/claude3_5_0_250.csv\")[[\"llm_indication\"]].rename(\n",
    "    columns={\"llm_indication\": \"llm_indication_claude\"}\n",
    ")\n",
    "\n",
    "llama_evaluation = pd.read_csv(\"../inference/results/reader_evaluation_dataset/Qwen_Qwen2.5-7B-Instruct_0_250.csv\")[[\"llm_indication\"]].rename(\n",
    "    columns={\"llm_indication\": \"llm_indication_qwen\"}\n",
    ")\n",
    "\n",
    "reader_evaluation_dataset = pd.concat([reader_evaluation_dataset, claude_evaluation, llama_evaluation], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d74c7128-93c1-41c9-99e8-a8f947fe3237",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 123\n",
    "indication_choices = [\n",
    "    \"original_history\", \n",
    "    \"additional_history\", \n",
    "    \"llm_indication_claude\",\n",
    "    \"llm_indication_qwen\"\n",
    "]\n",
    "def randomize_indications(_):\n",
    "    choices = indication_choices.copy()\n",
    "    np.random.shuffle(choices)\n",
    "    return pd.Series(choices, index=[\"indication1_random\", \"indication2_random\", \"indication3_random\", \"indication4_random\"])\n",
    "\n",
    "# Apply the function row-wise and assign the new columns to the DataFrame\n",
    "reader_evaluation_dataset[[\"indication1_choice\", \"indication2_choice\", \"indication3_choice\", \"indication4_choice\"]] = \\\n",
    "    reader_evaluation_dataset.apply(randomize_indications, axis=1)\n",
    "\n",
    "reader_evaluation_dataset.to_parquet(\"randomized_reader_study_evaluation.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407bbb3f-1b0a-4dac-b5be-8e0bd040607e",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Case Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b4286b9-ded7-404d-b2dd-56c74cb15ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create small groupings, this is necessary for uniform distribution to ensure equal group sizing (in setting of a rectangular matrix...)\n",
    "# we want 25 cases per person for 20 people  , 2 repeats of each case , so we want a dimension 10 matrix , 5 groupings of 3 and 5 groupings of 2\n",
    "X = list()\n",
    "for i in range(10):\n",
    "    row = list()\n",
    "    for j in range(10):\n",
    "        if (j - i) % 10 in [0, 1, 2, 3, 4]:\n",
    "            row.append(3)\n",
    "        else:\n",
    "            row.append(2)\n",
    "    X.append(row)\n",
    "\n",
    "# use X and our index array to create a matrix of the actual cases, grouped according to small groups defined in X - call this matrix Y.\n",
    "# essentially we are sampling from the case index (labeled index) and drawing # of cases according to matrix X\n",
    "# well its a \"matrix\" but really the structure is a list of lists. the first index will refer to the \"row\" and the second index will be the \"column\"\n",
    "# Y will be a square matrix by construction\n",
    "\n",
    "index = list(range(250))\n",
    "Y = list()\n",
    "start_index=0\n",
    "\n",
    "for i in range(10):\n",
    "  row = list()\n",
    "  for j in range(10):\n",
    "    row.append(index[start_index:start_index+X[i][j]])\n",
    "    #update start_index\n",
    "    start_index=start_index+X[i][j]\n",
    "  Y.append(row)\n",
    "\n",
    "# use matrix Y to create the final assignments by taking the union across each row to create the first set of 10 cases and then do the same by column for the second set of 10 cases store these as arrays\n",
    "\n",
    "case_assignments=list()\n",
    "\n",
    "#first assignment for first 10 raters, read along rows of Y\n",
    "for i in range(10):\n",
    "  case_assignments.append([element for row in Y[i] for element in row])\n",
    "\n",
    "#second assignment for raters 11-20, read along columns of Y, create array of arrays to store reworked values. unfortunately list of lists makes this hard so create a temporary matrix Y2 first which is a transpose of Y\n",
    "Y2= [Y[j][i] for i in range(10) for j in range(10)]\n",
    "\n",
    "for i in range(10):\n",
    "  case_assignments.append(([element for row in Y2[10*i:10*i+10] for element in row]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2f90e49-5f65-4db6-a078-b83721ae55ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:29<00:00,  1.49s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "NUM_READERS = 20\n",
    "WORDS_PER_LINE = 10 \n",
    "LINES_PER_BLOCK = 20\n",
    "\n",
    "\n",
    "for i in tqdm.tqdm(range(NUM_READERS)):\n",
    "    case_assignment = case_assignments[i]\n",
    "    reader_evaluation_dataset_subset = reader_evaluation_dataset.iloc[case_assignment].reset_index(drop=True)\n",
    "    for j in range(len(reader_evaluation_dataset_subset)):\n",
    "        row = reader_evaluation_dataset_subset.iloc[j]\n",
    "        basepath = f\"/mnt/sohn2022/Adrian/rad-llm-pmhx/dataset/reader_study/evaluation/public/user{i+1}/set{j+1}\"\n",
    "        os.makedirs(basepath, exist_ok=True)\n",
    "        \n",
    "        with open(f\"{basepath}/exam.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(str(row[\"exam_type\"]))\n",
    "        \n",
    "        note_titles = row[\"note_titles\"]\n",
    "        note_texts = row[\"note_texts\"]\n",
    "        \n",
    "        for k in range(10):\n",
    "            filename = f\"{basepath}/note{k+1}.txt\"\n",
    "            with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "                if k < len(note_titles) and k < len(note_texts):\n",
    "                    f.write(note_titles[k] + \"\\n\")\n",
    "                    words = note_texts[k].split()\n",
    "                    line_count = 0\n",
    "                    for idx in range(0, len(words), WORDS_PER_LINE):\n",
    "                        line = \" \".join(words[idx:idx + WORDS_PER_LINE])\n",
    "                        f.write(line + \"\\n\")\n",
    "                        line_count += 1\n",
    "                        if line_count % LINES_PER_BLOCK == 0:\n",
    "                            f.write(\"\\n\") \n",
    "                else:\n",
    "                    f.write(\"\")\n",
    "        \n",
    "        for l in range(4):\n",
    "            filename = f\"{basepath}/indication{l+1}.txt\"\n",
    "            with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(row[row[f\"indication{l+1}_choice\"]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "report_gen",
   "language": "python",
   "name": "report_gen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
